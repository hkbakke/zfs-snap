# Override global defaults for all policies
#defaults:
#  cmds:
#    ssh: /usr/bin/ssh
#    zfs: /sbin/zfs
#    split: /usr/bin/split
#    cat: /bin/cat
#  keep:
#    latest: 0
#    hourly: 0
#    daily: 0
#    weekly: 0
#    monthly: 0
#    yearly: 0


#
# EXAMPLES
#

policies:
  all-but-backup:
    type: snapshot

    # Label defaults to policy name if not set (this is valid for all policies)
    label: hourly

    # If you leave 'include' unset, all datasets will be snapshoted by default
    include:
      - '*'
    exclude:
      - 'pool-2/backup'

    # If keep is not set global defaults are used
    keep:
      hourly: 24
      daily: 7
      weekly: 5

    # If not set recursive defaults to 'no'
    #recursive: no

    # Override cmd paths
    #cmds:
    #  zfs: /some/path

  # A minimal config version that snapshots all datasets with no excludes
  # non-recursively
  daily:
    type: snapshot
    keep:
      daily: 31

  # You can have many include and exclude patterns
  some-pools-only:
    type: snapshot
    include:
      - 'pool-1*'
      - 'pool-2*'
    exclude:
      - 'pool-1/backup'
      - 'pool-2/backup'
    keep:
      daily: 31

  recursive:
    # Here we are doing recursive snapshots on the top level, excluding all
    # lower level datasets as they are included in the top level recursive
    # snapshots.
    type: snapshot
    include:
      - '*'
    exclude:
      - '*/*'
    keep:
      hourly: 24
    recursive: yes

  replicate-vms:
    # The destination dataset _must_ be unused, preferable not even created,
    # as zfssnap will ensure it is looking like the source, and even destroying
    # the remote dataset recursively if --reset is given, to ensure compliance.
    type: replicate
    source:
      dataset: 'pool-1/vms-1'
      #cmds:
      #  zfs: /path/to/zfs
      #  ssh: /path/to/ssh
    destination:
      dataset: 'pool-2/backup/vms-1'

      # Leave unset if local destination dataset. Not needed if the remote user
      # is the same as the user running the script.
      ssh_user: root

      # Leave unset if local destination dataset
      host: remotehost

      # The destination dataset is made read only by default as it really doesn't
      # make sense to allow writes as long as it is replicated to
      #read_only: yes

      #cmds:
      #  zfs: /path/to/zfs
    keep:
      # One snapshot is always kept for replication policies to ensure
      # incremental send is possible, regardless of these settings.
      hourly: 24
      daily: 7
      weekly: 5

  # Example configuration for the sending side when replicating a dataset by
  # file. If the send output exceeds split_size a new segment is generated.
  # This is to be able to start transferring segments from the destination dir
  # before having completed the whole send operation, which could save a lot of
  # time if the send datastream becomes large.
  send-to-file:
    type: send_to_file
    label: replicate-disconnected

    # If not set file_prefix defaults to 'zfssnap'
    file_prefix: xyz

    # If not set split_size defaults to '1G'
    split_size: 512M

    # If not set suffix_length defaults to 4
    suffix_length: 4
    source:
       dataset: 'pool-1/vms-1'
    destination:
       dir: /srv/outgoing
    #cmds:
    #  zfs: /path/to/zfs
    #  split: /path/to/split
    keep:
      # One snapshot is always kept for replication policies to ensure
      # incremental send is possible, regardless of these settings.
      latest: 4
      hourly: 24
      daily: 31

  # Example configuration for the receiving side when replicating a dataset by
  # file. The file_prefix and label must match that of the sending side.
  # There is no keep policy for the receiving end as this is handled by the
  # sending side.
  receive-from-file:
    type: receive_from_file
    label: replicate-disconnected
    file_prefix: xyz
    source:
      dir: /srv/incoming
    destination:
      dataset: 'pool-1/backup/test-1'
      #read_only: yes
    #cmds:
    #  zfs: /path/to/zfs
    #  cat: /path/to/cat
